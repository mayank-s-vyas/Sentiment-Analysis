{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "353b7fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f84a4b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(152, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>CONAME</th>\n",
       "      <th>FYRMO</th>\n",
       "      <th>FDATE</th>\n",
       "      <th>FORM</th>\n",
       "      <th>SECFNAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199803</td>\n",
       "      <td>1998-03-06</td>\n",
       "      <td>10-K405</td>\n",
       "      <td>edgar/data/3662/0000950170-98-000413.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199805</td>\n",
       "      <td>1998-05-15</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>edgar/data/3662/0000950170-98-001001.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199808</td>\n",
       "      <td>1998-08-13</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-000783.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-12</td>\n",
       "      <td>10-K/A</td>\n",
       "      <td>edgar/data/3662/0000950170-98-002145.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-16</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-001203.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CIK            CONAME   FYRMO      FDATE     FORM  \\\n",
       "0  3662  SUNBEAM CORP/FL/  199803 1998-03-06  10-K405   \n",
       "1  3662  SUNBEAM CORP/FL/  199805 1998-05-15     10-Q   \n",
       "2  3662  SUNBEAM CORP/FL/  199808 1998-08-13  NT 10-Q   \n",
       "3  3662  SUNBEAM CORP/FL/  199811 1998-11-12   10-K/A   \n",
       "4  3662  SUNBEAM CORP/FL/  199811 1998-11-16  NT 10-Q   \n",
       "\n",
       "                                   SECFNAME  \n",
       "0  edgar/data/3662/0000950170-98-000413.txt  \n",
       "1  edgar/data/3662/0000950170-98-001001.txt  \n",
       "2  edgar/data/3662/0000950172-98-000783.txt  \n",
       "3  edgar/data/3662/0000950170-98-002145.txt  \n",
       "4  edgar/data/3662/0000950172-98-001203.txt  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read excel file 'cik_list' \n",
    "cik_data = pd.read_excel('cik_list.xlsx')\n",
    "print(cik_data.shape)\n",
    "cik_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bacc3ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating a column containing full link of the report for corresponding row\n",
    "cik_data['link'] = 'https://www.sec.gov/Archives/' + cik_data['SECFNAME'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4899b404",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n"
     ]
    }
   ],
   "source": [
    "# https://pypi.org/project/requests-random-user-agent/\n",
    "#this was required as I was not able to get reports from site do request limit exceeded\n",
    "import requests_random_user_agent\n",
    "\n",
    "requests.Session()\n",
    "\n",
    "reports = []\n",
    "for link in cik_data.link:\n",
    "    l = requests.get(link)\n",
    "    report = l.text\n",
    "    soup = BeautifulSoup(report, \"html.parser\")\n",
    "    reports.append(soup.get_text())\n",
    "                   \n",
    "print(len(reports))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dceaf4e",
   "metadata": {},
   "source": [
    "### Cleaning Using Stopwords Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61b78320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n"
     ]
    }
   ],
   "source": [
    "#read stopwords text and save it to a list stopwords \n",
    "with open('StopWords_Generic.txt','r') as x:\n",
    "    StopWords = x.read()\n",
    "StopWords = StopWords.split('\\n')\n",
    "print(len(StopWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c390e4",
   "metadata": {},
   "source": [
    "There are 121 stop words in the list provided by SRAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8dd3cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for removing stopwords\n",
    "def rem_stopwords(words, StopWords):\n",
    "    not_stopwords = []\n",
    "    for i in words:\n",
    "        if i not in StopWords:\n",
    "            not_stopwords.append(i)\n",
    "    return not_stopwords\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3467c078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/47091490/4084039\n",
    "#to preprocess text data, removing extra characters and expressions. Expanding contracted words like we'll to we will\n",
    "def preprocessed(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    phrase = re.sub(r\"\\\\r\\\\n\", \"\", phrase)\n",
    "    phrase = re.sub('[\\d%/$]', '', phrase) #to remove digits\n",
    "    phrase = phrase.replace('\\\\r', ' ') # \\r, \\n , \\t remove from string\n",
    "    phrase = phrase.replace('\\\\\"', ' ')\n",
    "    phrase = phrase.replace('\\\\n', ' ')\n",
    "    phrase = re.sub('[^A-Za-z0-9]+', ' ', phrase) #remove special character\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c86c909f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 152/152 [00:20<00:00,  7.54it/s]\n"
     ]
    }
   ],
   "source": [
    "#removing stopwords from the text in reports\n",
    "from tqdm import tqdm\n",
    "texts_without_stopwords = []\n",
    "# tqdm is for printing the status bar\n",
    "for i in tqdm(range(len(reports))):\n",
    "    text = reports[i]\n",
    "    text = preprocessed(text)\n",
    "    text = ' '.join(e for e in text.upper().split() if e not in StopWords)\n",
    "    texts_without_stopwords.append(text.strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fddb638",
   "metadata": {},
   "source": [
    "###  1.2 Creating Dictionary of Positive and Negative Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6183d91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86531, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Seq_num</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Word Proportion</th>\n",
       "      <th>Average Proportion</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Doc Count</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Uncertainty</th>\n",
       "      <th>Litigious</th>\n",
       "      <th>Strong_Modal</th>\n",
       "      <th>Weak_Modal</th>\n",
       "      <th>Constraining</th>\n",
       "      <th>Complexity</th>\n",
       "      <th>Syllables</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AARDVARK</td>\n",
       "      <td>1</td>\n",
       "      <td>312</td>\n",
       "      <td>1.422050e-08</td>\n",
       "      <td>1.335201e-08</td>\n",
       "      <td>3.700747e-06</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AARDVARKS</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.367356e-10</td>\n",
       "      <td>8.882163e-12</td>\n",
       "      <td>9.362849e-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word  Seq_num  Word Count  Word Proportion  Average Proportion  \\\n",
       "0   AARDVARK        1         312     1.422050e-08        1.335201e-08   \n",
       "1  AARDVARKS        2           3     1.367356e-10        8.882163e-12   \n",
       "\n",
       "        Std Dev  Doc Count  Negative  Positive  Uncertainty  Litigious  \\\n",
       "0  3.700747e-06         96         0         0            0          0   \n",
       "1  9.362849e-09          1         0         0            0          0   \n",
       "\n",
       "   Strong_Modal  Weak_Modal  Constraining  Complexity  Syllables     Source  \n",
       "0             0           0             0           0          2  12of12inf  \n",
       "1             0           0             0           0          2  12of12inf  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading master dictionary.xlsx file of LoughranMcDonald\n",
    "master_dict = pd.read_excel('LoughranMcDonald_MasterDictionary_2020.xlsx')\n",
    "print(master_dict.shape)\n",
    "master_dict.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6283118d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO. of positive words: 354\n",
      "NO. of negative words: 2355\n"
     ]
    }
   ],
   "source": [
    "#positive and negative word dictionary of words that are not in stopwords\n",
    "PositiveWords = [x for x in master_dict[master_dict['Positive'] != 0]['Word']]\n",
    "pos_dict = [i for i in PositiveWords if i not in StopWords]\n",
    "print('NO. of positive words:',len(pos_dict))\n",
    " \n",
    "NegativeWords = [x for x in master_dict[master_dict['Negative'] != 0]['Word']]\n",
    "neg_dict = [i for i in NegativeWords if i not in StopWords]\n",
    "print('NO. of negative words:',len(neg_dict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef589560",
   "metadata": {},
   "source": [
    "### Extracting Derived Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d6e5873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for tokenizing the text report and reports after getting tokenized\n",
    "def tokenize(text):\n",
    "    TokenWords = word_tokenize(text) #tokenizer\n",
    "    return TokenWords\n",
    "\n",
    "tokenized_reports = list(map(tokenize, texts_without_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2775934d",
   "metadata": {},
   "source": [
    "### Positive Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "718031b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 152/152 [00:30<00:00,  4.96it/s]\n"
     ]
    }
   ],
   "source": [
    "#calculate positive_score i.e, no. of positive words in the report\n",
    "positive_score = []\n",
    "for i in tqdm(tokenized_reports):\n",
    "    pos_score_i = 0\n",
    "    for x in i:\n",
    "        if x in pos_dict:\n",
    "            pos_score_i += 1\n",
    "    positive_score.append(pos_score_i)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da907b2",
   "metadata": {},
   "source": [
    "### Negative Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1998b0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 152/152 [03:47<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "#calculate negative_score i.e, no. of negative words in the report\n",
    "negative_score = []\n",
    "for i in tqdm(tokenized_reports):\n",
    "    neg_score_i = 0\n",
    "    for x in i:\n",
    "        if x in neg_dict:\n",
    "            neg_score_i -= 1\n",
    "    neg_score_i = -neg_score_i\n",
    "    negative_score.append(neg_score_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7973263",
   "metadata": {},
   "source": [
    "### Polarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24918fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#polarity score = (positive score - negative score)/((positive score + negative score)+0.000001)\n",
    "polarity_score = [(positive_score[i] - negative_score[i])/((positive_score[i] + negative_score[i])+ 0.000001) for i in range(len(positive_score))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05ad3974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for sentence tokenizing the text report and tokenized reports\n",
    "def tokenize_sent(text):\n",
    "    token_sent = sent_tokenize(text) #sentence tokenizer\n",
    "    return token_sent\n",
    "\n",
    "tokenized__sent_reports = list(map(tokenize_sent, reports))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19a52ed",
   "metadata": {},
   "source": [
    "### Average Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b218d694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#average sentence length\n",
    "avg_sent_length = [(len(tokenized_reports[i]))/(len(tokenized__sent_reports[i])) for i in range(len(tokenized_reports))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5a49cd",
   "metadata": {},
   "source": [
    "### Complex Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2846d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for complex word count\n",
    "\n",
    "def complex_word_count(text):\n",
    "    complex_word = 0\n",
    "    for word in text:\n",
    "        count = 0\n",
    "        vowels = ['a','e','i','o','u']\n",
    "        if not (len(word) > 2 and (word[-2:] == 'ES' or word[-2:] == 'ED')): # removing exceptions        \n",
    "            for x in word:\n",
    "                if(x.lower() in vowels):\n",
    "                    count = count +1\n",
    "                        \n",
    "        if(count > 2):\n",
    "            complex_word += 1\n",
    "    return complex_word \n",
    "\n",
    "#complex word count\n",
    "complex_word_count = list(map(complex_word_count, tokenized_reports))          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339cfb2d",
   "metadata": {},
   "source": [
    "### Percentage of Complex Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ea3fd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage of complex words\n",
    "per_complex_words = [(complex_word_count[i]/len(tokenized_reports[i])) for i in range(len(tokenized_reports))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c57479",
   "metadata": {},
   "source": [
    "### Fog Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85c19058",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fog index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
    "def fog_index(average_sentance_length , percentage_complex_words):\n",
    "    return 0.4*(average_sentance_length + percentage_complex_words)\n",
    "fog_ind = list(map(fog_index, avg_sent_length, per_complex_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1189fc3",
   "metadata": {},
   "source": [
    "### Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1fa0c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word caount, already cleaned words(removed stopwords and punctuations) in tokenized words\n",
    "word_count = [len(i) for i in tokenized_reports]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33cb3fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading excel file of uncertainity and constraining dictionary\n",
    "uncertainty_dict = pd.read_excel('uncertainty_dictionary.xlsx')\n",
    "constraining_dict = pd.read_excel('constraining_dictionary.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeed7112",
   "metadata": {},
   "source": [
    "### Uncertainity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7f15383",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 152/152 [03:43<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "#uncertainity score is no. of words found in reports which are available in uncertainity dictionary\n",
    "uncertainity = []\n",
    "for i in tqdm(tokenized_reports):\n",
    "    count = 0\n",
    "    for x in i:\n",
    "        if x in list(uncertainty_dict['Word']):\n",
    "            count += 1\n",
    "    uncertainity.append(count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877717ff",
   "metadata": {},
   "source": [
    "### Constraining Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d711da50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 152/152 [02:30<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "#constraining score is no. of words found in reports which are available in constraining dictionary\n",
    "constraining = []\n",
    "for i in tqdm(tokenized_reports):\n",
    "    count = 0\n",
    "    for x in i:\n",
    "        if x in list(constraining_dict['Word']):\n",
    "            count += 1\n",
    "    constraining.append(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57fad2d",
   "metadata": {},
   "source": [
    "### Positive, Negative, Uncertainty & Constraining Word Proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ba36005",
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive word proportion\n",
    "pos_word_prop = [(positive_score[i]/len(tokenized_reports[i])) for i in range(len(tokenized_reports))]\n",
    "\n",
    "#negative word proportion\n",
    "neg_word_prop = [(negative_score[i]/len(tokenized_reports[i])) for i in range(len(tokenized_reports))]\n",
    "\n",
    "#uncertainty word proportion\n",
    "uncertainty_word_prop = [(uncertainity[i]/len(tokenized_reports[i])) for i in range(len(tokenized_reports))]\n",
    "\n",
    "#constraining word proportion score\n",
    "constraining_word_prop = [(constraining[i]/len(tokenized_reports[i])) for i in range(len(tokenized_reports))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f89a6b",
   "metadata": {},
   "source": [
    "###  Constraining Words for Whole report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "166249e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraining Words for Whole report, which is similar to constraining score\n",
    "constraining_words_report = constraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c90d539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(152, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>CONAME</th>\n",
       "      <th>FYRMO</th>\n",
       "      <th>FDATE</th>\n",
       "      <th>FORM</th>\n",
       "      <th>SECFNAME</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>polarity_score</th>\n",
       "      <th>average_sentence_length</th>\n",
       "      <th>...</th>\n",
       "      <th>fog_index</th>\n",
       "      <th>complex_word_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>uncertainty_score</th>\n",
       "      <th>constraining_score</th>\n",
       "      <th>positive_word_proportion</th>\n",
       "      <th>negative_word_proportion</th>\n",
       "      <th>uncertainty_word_proportion</th>\n",
       "      <th>constraining_word_proportion</th>\n",
       "      <th>constraining_words_whole_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199803</td>\n",
       "      <td>1998-03-06</td>\n",
       "      <td>10-K405</td>\n",
       "      <td>edgar/data/3662/0000950170-98-000413.txt</td>\n",
       "      <td>1065</td>\n",
       "      <td>2997</td>\n",
       "      <td>-0.475628</td>\n",
       "      <td>23.506687</td>\n",
       "      <td>...</td>\n",
       "      <td>9.569851</td>\n",
       "      <td>38934</td>\n",
       "      <td>93157</td>\n",
       "      <td>940</td>\n",
       "      <td>1487</td>\n",
       "      <td>0.011432</td>\n",
       "      <td>0.032171</td>\n",
       "      <td>0.010090</td>\n",
       "      <td>0.015962</td>\n",
       "      <td>1487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199805</td>\n",
       "      <td>1998-05-15</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>edgar/data/3662/0000950170-98-001001.txt</td>\n",
       "      <td>585</td>\n",
       "      <td>1477</td>\n",
       "      <td>-0.432590</td>\n",
       "      <td>24.213942</td>\n",
       "      <td>...</td>\n",
       "      <td>9.858303</td>\n",
       "      <td>26098</td>\n",
       "      <td>60438</td>\n",
       "      <td>859</td>\n",
       "      <td>1046</td>\n",
       "      <td>0.009679</td>\n",
       "      <td>0.024438</td>\n",
       "      <td>0.014213</td>\n",
       "      <td>0.017307</td>\n",
       "      <td>1046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199808</td>\n",
       "      <td>1998-08-13</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-000783.txt</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>29.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>11.754940</td>\n",
       "      <td>196</td>\n",
       "      <td>581</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.003442</td>\n",
       "      <td>0.013769</td>\n",
       "      <td>0.015491</td>\n",
       "      <td>0.008606</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-12</td>\n",
       "      <td>10-K/A</td>\n",
       "      <td>edgar/data/3662/0000950170-98-002145.txt</td>\n",
       "      <td>368</td>\n",
       "      <td>1442</td>\n",
       "      <td>-0.593370</td>\n",
       "      <td>18.954509</td>\n",
       "      <td>...</td>\n",
       "      <td>7.755565</td>\n",
       "      <td>20453</td>\n",
       "      <td>47083</td>\n",
       "      <td>552</td>\n",
       "      <td>716</td>\n",
       "      <td>0.007816</td>\n",
       "      <td>0.030627</td>\n",
       "      <td>0.011724</td>\n",
       "      <td>0.015207</td>\n",
       "      <td>716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-16</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-001203.txt</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.454545</td>\n",
       "      <td>27.880000</td>\n",
       "      <td>...</td>\n",
       "      <td>11.279977</td>\n",
       "      <td>223</td>\n",
       "      <td>697</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.004304</td>\n",
       "      <td>0.011478</td>\n",
       "      <td>0.014347</td>\n",
       "      <td>0.005739</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    CIK            CONAME   FYRMO      FDATE     FORM  \\\n",
       "0  3662  SUNBEAM CORP/FL/  199803 1998-03-06  10-K405   \n",
       "1  3662  SUNBEAM CORP/FL/  199805 1998-05-15     10-Q   \n",
       "2  3662  SUNBEAM CORP/FL/  199808 1998-08-13  NT 10-Q   \n",
       "3  3662  SUNBEAM CORP/FL/  199811 1998-11-12   10-K/A   \n",
       "4  3662  SUNBEAM CORP/FL/  199811 1998-11-16  NT 10-Q   \n",
       "\n",
       "                                   SECFNAME  positive_score  negative_score  \\\n",
       "0  edgar/data/3662/0000950170-98-000413.txt            1065            2997   \n",
       "1  edgar/data/3662/0000950170-98-001001.txt             585            1477   \n",
       "2  edgar/data/3662/0000950172-98-000783.txt               2               8   \n",
       "3  edgar/data/3662/0000950170-98-002145.txt             368            1442   \n",
       "4  edgar/data/3662/0000950172-98-001203.txt               3               8   \n",
       "\n",
       "   polarity_score  average_sentence_length  ...  fog_index  \\\n",
       "0       -0.475628                23.506687  ...   9.569851   \n",
       "1       -0.432590                24.213942  ...   9.858303   \n",
       "2       -0.600000                29.050000  ...  11.754940   \n",
       "3       -0.593370                18.954509  ...   7.755565   \n",
       "4       -0.454545                27.880000  ...  11.279977   \n",
       "\n",
       "   complex_word_count  word_count  uncertainty_score  constraining_score  \\\n",
       "0               38934       93157                940                1487   \n",
       "1               26098       60438                859                1046   \n",
       "2                 196         581                  9                   5   \n",
       "3               20453       47083                552                 716   \n",
       "4                 223         697                 10                   4   \n",
       "\n",
       "   positive_word_proportion  negative_word_proportion  \\\n",
       "0                  0.011432                  0.032171   \n",
       "1                  0.009679                  0.024438   \n",
       "2                  0.003442                  0.013769   \n",
       "3                  0.007816                  0.030627   \n",
       "4                  0.004304                  0.011478   \n",
       "\n",
       "   uncertainty_word_proportion  constraining_word_proportion  \\\n",
       "0                     0.010090                      0.015962   \n",
       "1                     0.014213                      0.017307   \n",
       "2                     0.015491                      0.008606   \n",
       "3                     0.011724                      0.015207   \n",
       "4                     0.014347                      0.005739   \n",
       "\n",
       "   constraining_words_whole_report  \n",
       "0                             1487  \n",
       "1                             1046  \n",
       "2                                5  \n",
       "3                              716  \n",
       "4                                4  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating columns in the cik list of the variables as required\n",
    "cik_data['positive_score'] = positive_score\n",
    "cik_data['negative_score'] = negative_score\n",
    "cik_data['polarity_score'] = polarity_score\n",
    "cik_data['average_sentence_length'] = avg_sent_length\n",
    "cik_data['percentage_of_complex_words'] = per_complex_words\n",
    "cik_data['fog_index'] = fog_ind\n",
    "cik_data['complex_word_count'] = complex_word_count\n",
    "cik_data['word_count'] = word_count\n",
    "cik_data['uncertainty_score'] = uncertainity\n",
    "cik_data['constraining_score'] = constraining\n",
    "cik_data['positive_word_proportion'] = pos_word_prop\n",
    "cik_data['negative_word_proportion'] = neg_word_prop\n",
    "cik_data['uncertainty_word_proportion'] = uncertainty_word_prop\n",
    "cik_data['constraining_word_proportion'] = constraining_word_prop\n",
    "cik_data['constraining_words_whole_report'] = constraining_words_report\n",
    "output_data = cik_data.drop('link', axis = 1)\n",
    "print(output_data.shape)\n",
    "output_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7275b553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting csv file for the output of output_data dataframe\n",
    "output_data.to_csv('Output.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b96ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#polarity score gives info about if a text is negative or positive\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
